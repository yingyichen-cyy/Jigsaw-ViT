 <!DOCTYPE html>
<html lang="en">
<head>
  <title>Jigsaw-ViT</title>
  <meta name="description" content="Project page for Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!--Facebook preview-->
  <meta property="og:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="600">
  <meta property="og:image:height" content="400">
  <meta property="og:type" content="website"/>
  <meta property="og:url" content="https://imagine.enpc.fr/~monniert/DTIClustering/"/>
  <meta property="og:title" content="DTI Clustering"/>
  <meta property="og:description" content="Project page for Deep Transformation-Invariant Clustering."/>

  <!--Twitter preview-->
  <meta name="twitter:card" content="summary_large_image" />
  <meta name="twitter:title" content="DTI Clustering" />
  <meta name="twitter:description" content="Project page for Deep Transformation-Invariant Clustering."/>
  <meta name="twitter:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail_twitter.png">

  <!--Style-->
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>

</head>
<body>

<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer</h1>
    <!-- <h4>Preprint</h4> -->
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-2"></div>
    <div class="col-xs-12 col-md-8">
      <h4>
        <a href="https://github.com/yingyichen-cyy"><nobr>Yingyi Chen</nobr></a><sup>1</sup> &emsp;
        <a href="https://xishen0220.github.io/"><nobr>Xi Shen</nobr></a><sup>2</sup> &emsp;
        <a href="https://yhlleo.github.io/"><nobr>Yahui Liu</nobr></a><sup>3</sup> &emsp;
        <a href="https://www.esat.kuleuven.be/stadius/person.php?id=2167"><nobr>Qinghua Tao</nobr></a><sup>1</sup> &emsp;
        <a href="https://www.esat.kuleuven.be/sista/members/suykens.html"><nobr>Johan A.K. Suykens</nobr></a><sup>1</sup>
        
      </h4>

      <sup>1</sup> ESAT-STADIUS, KU Leuven, Leuven &emsp;
      <sup>2</sup> Tencent AI Lab, Shenzhen &emsp;
      <sup>3</sup> University of Trento, Trento
    </div>
  </div>
</div>

<div class="container" style="text-align:center; padding:1rem">
  <img src="resrc/teaser.jpg" alt="teaser.jpg" class="text-center" style="width: 60%; max-width: 1100px">
  <br/>
  <br/>
  <p><b>Overview framework of our Jigsaw-ViT.</b> 
  (Top) We incorporate jigsaw puzzle solving (in <b><font color=#0080ff>blue</font></b> flow) into the standard ViT for image classification (in <b><font color=#ff4000>red</font></b>). During the training, we jointly learn the two tasks.
  (Bottom) The details of our jigsaw puzzle flow. We drop several patches, i.e., patch masking, and remove positional embeddings before feeding to ViT. For each unmasked patch, the model should predict the class corresponding to the patch position.
  </p>
  <h3 style="text-align:center; padding-top:1rem">
    <!-- <a class="label label-info" href="">arXiv</a> -->
    <a class="label label-info" href="https://arxiv.org/abs/2207.11971">Paper</a>
    <a class="label label-info" href="https://github.com/yingyichen-cyy/JigsawViT">Code</a>
  </h3>
</div>

<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>
    The success of Vision Transformer (ViT) in various computer vision tasks has promoted the ever-increasing prevalence of this convolution-free network.
    The fact that ViT works on image patches makes it potentially relevant to the problem of jigsaw puzzle solving, which is a classical self-supervised task aiming at reordering shuffled sequential image patches back to their natural form. Despite its simplicity, solving jigsaw puzzle has been demonstrated to be helpful for diverse tasks using Convolutional Neural Networks (CNNs), such as self-supervised feature representation learning, domain generalization, and fine-grained classification. 
    <br/> 
    <br/> 
    In this paper, we explore solving jigsaw puzzle as a self-supervised auxiliary loss in ViT for image classification, named Jigsaw-ViT. We show two modifications that can make Jigsaw-ViT superior to standard ViT: discarding positional embeddings and masking patches randomly. Yet simple, we find that Jigsaw-ViT is able to improve both in generalization and robustness over the standard ViT, which is usually rather a trade-off. Experimentally, we show that adding the jigsaw puzzle branch provides better generalization than ViT on large-scale image classification on ImageNet. Moreover, the auxiliary task also improves robustness to noisy labels on Animal-10N, Food-101N, and Clothing1M as well as adversarial examples.
  </p>

  <h3>Method</h3>
  <hr/>
  <p>
    The overview framework of our approach is illustrated in the teaser image. 
    Precisely, we focus on image classification problem using ViT. 
    Our goal is to train a model that solves the standard classification and jigsaw puzzles simultaneously. 
    The total loss <img src="http://latex.codecogs.com/svg.latex?\mathcal{L}_{\text{total}}" alt="\mathcal{L}_{\text{total}}" border="0"/> is a simple weighted combination of two cross-entropy losses denoted by <img src="http://latex.codecogs.com/svg.latex?CE" alt="CE" border="0"/>s: 
    the class prediction loss on the <img src="http://latex.codecogs.com/svg.latex?CLS" alt="CLS" border="0"/> token <img src="http://latex.codecogs.com/svg.latex?\mathcal{L}_{\text{cls}}" alt="\mathcal{L}_{\text{cls}}" border="0"/> and the position prediction loss on the patch tokens <img src="http://latex.codecogs.com/svg.latex?\mathcal{L}_{\text{jigsaw}}" alt="\mathcal{L}_{\text{jigsaw}}" border="0"/>:
    <br/>
    <br/>
    <center>
    <img src="http://latex.codecogs.com/svg.latex?\mathcal{L}_{\text{total}} = \underbrace{CE(\mathbf{y}_{\text{pred}}, y)}_{\mathcal{L}_{\text{cls}}} + \eta \underbrace{CE(\tilde{\mathbf{y}}_{\text{pred}}, \tilde{y})}_{\mathcal{L}_{\text{jigsaw}}}" alt="\mathcal{L}_{\text{total}} = \underbrace{CE(\mathbf{y}_{\text{pred}}, y)}_{\mathcal{L}_{\text{cls}}} + \eta \underbrace{CE(\tilde{\mathbf{y}}_{\text{pred}}, \tilde{y})}_{\mathcal{L}_{\text{jigsaw}}}" border="0"/>
    </center>
    <br/>
    where <img src="http://latex.codecogs.com/svg.latex?\tilde{\mathbf{y}}_{\text{pred}}" alt="\tilde{\mathbf{y}}_{\text{pred}}" border="0"/> and <img src="http://latex.codecogs.com/svg.latex?\tilde{y}" alt="\tilde{y}" border="0"/> denote position prediction and the corresponding real position, respectively, and <img src="http://latex.codecogs.com/svg.latex?\eta" alt="\eta" border="0"/> is a hyper-parameter to balance the two losses.
    <br/>
    <br/>
    Our jigsaw puzzle flow is detailed in bottom part of the teaser image.
    Specifically, we include two changes compared to naive jigsaw puzzle implementation: 
    <i>(i)</i> we get rid of positional embedding in the jigsaw puzzle flow, which provides explicit clues to the position prediction; 
    <i>(ii)</i> we randomly drop <img src="http://latex.codecogs.com/svg.latex?\gamma L" alt="\gamma L" border="0"/> 
    patches in the input where 
    <img src="http://latex.codecogs.com/svg.latex?\gamma\in[0,1)" alt="\gamma\in[0,1)" border="0"/> 
    is a hyper-parameter denoting the mask ratio. 
    It can be apparently seen that both two strategies increase the difficulties in solving jigsaw puzzle,
    which is demonstrated to be helpful for the main classification task (Section 4 in the paper). 
    Since the proposed jigsaw puzzle solving is a self-supervised task, it can be easily plugged into existing ViT without modifying the original architecture.
  </p>


  <h3>Results</h3>
  <hr/>
  <p><font color=#ffa64d>Please refer to our paper for more experiments.</font></p>
  <p><h4 style="text-align: center"><u>Robustness to label noise</u></h4></p>
  <div class="row" style="text-align: center">
    <div class="col-xs-6">
      <h4 style="text-align: center"><u>Image classification on datasets with low noise rate</u></h4>
    </div>
    <div class="col-xs-6">
      <h4><u>Image classification on datasets with high noise rate</u></h4>
    </div>
  </div>
  <div class="row" style="text-align: center">
    <div class="col-xs-6">
      <img src="resrc/low-noise.jpg" alt="low-noise.jpg" class="text-center" style="width: 100%; max-width: 1000px">
    </div>
    <div class="col-xs-6">
      <img src="resrc/high-noise.jpg" alt="high-noise.jpg" class="text-center" style="width: 100%; max-width: 1000px; margin-top:
      10px">
    </div>
  </div>
  <div class="row" style="text-align: center">
    <div class="col-xs-6">
      <div style="width: 100%; max-width: 900px; padding-top:10px">
      <p><b>Image classification on datasets with low noise rate.</b> 
      We compare to state-of-the-art approaches and report test top-1 accuracy (%) on Animal-10N (noise ratio ~8%, in Table (a)) Food-101N (noise ratio ~20%, in Table (b)). 
      We also show how much our Jigsaw-ViT model is above or below DeiT-Small/16, which are marked with <b><font color=#009933>&uarr;</font></b> 
      and <b><font color=#e63900>&darr;</font></b> respectively.</p>
      </div>
    </div>
    <div class="col-xs-6">
      <div style="width: 100%; max-width: 900px; padding-top:10px">
        <p><b>Image classification on datasets with high noise rate.</b>  
        We compare to state-of-the-art approaches and report test top-1 accuracy (%) on Clothing1M (noise ratio ~38%). 
        We also show how much Jigsaw-ViT+NCT is above or below NCT, which are marked with <b><font color=#009933>&uarr;</font></b> 
        and <b><font color=#e63900>&darr;</font></b> respectively.</p>
    </div>
  </div>
  <dir>
    </br></br></br></br></br>
  </div>
  <div class="row" style="text-align: center">
  <p><h4 style="text-align: center"><u>Robustness to adversarial examples</u></h4></p>
  <img src="resrc/adv.jpg" alt="adv.jpg" class="text-center" style="width: 70%; max-width: 1000px; margin-top:10px">
        <p><b>Robustness to adversarial examples.</b> 
        We report top-1 accuracy (%) after attacks on ImageNet validation set (higher numbers indicate better model robustness). 
        (a) Attacks crafted by different methods in the black-box setting.
        (b) Weak white-box attacks.
        We also show how much our Jigsaw-ViT model is above or below DeiT-Small/16, which are marked with <b><font color=#009933>&uarr;</font></b> 
        and <b><font color=#e63900>&darr;</font></b> respectively.</p>
  </div>


  <h3>Resources</h3>
  <hr/>
  <div class="row" style="text-align: center">
    <div class="col-xs-2 col-lg-2"></div>
    <div class="col-xs-4 col-lg-4">
      <h4>Paper</h4>
      <a href="https://arxiv.org/abs/2207.11971" style="color:inherit"> 
        <img src="resrc/paper.jpg" alt="paper.jpg" class="text-center" style="max-width:70%; border:0.15em solid;
        border-radius:0.5em;"></a>
    </div>
    <div class="col-xs-4 col-lg-4">
      <h4>Code</h4>
      <a href="https://github.com/yingyichen-cyy/JigsawViT" style="color:inherit;">
        <img src="resrc/github_repo.jpg" alt="github_repo.jpg" class="text-center"
             style="max-width:70%; border:0.15em solid;border-radius:0.5em;"></a>
    </div>
  </div>

    <h4 style="padding-top:0.5em">BibTeX</h4>
    If you find this work useful for your research, please cite:
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
          @ARTICLE{chen2022jigsaw,
              author={Chen, Yingyi and Shen, Xi and Liu, Yahui and Tao, Qinghua and Suykens, Johan A. K.},
              journal={arXiv preprint arXiv:2207.11971}, 
              title={Jigsaw-ViT: Learning Jigsaw Puzzles in Vision Transformer}, 
              year={2022}
          }</pre>
      </div>
    </div>

  <h3>Acknowledgements</h3>
  <hr/>
  <p>
    This work is jointly supported by EU: The research leading to these results has received funding from the European Research Council under the European Union's Horizon 2020 research and innovation program / ERC Advanced Grant E-DUALITY (787960). This paper reflects only the authors' views and the Union is not liable for any use that may be made of the contained information.
    Research Council KU Leuven:
    Optimization frameworks for deep kernel machines C14/18/068
    Flemish Government:
    FWO: projects: GOA4917N (Deep Restricted Kernel Machines: Methods and Foundations), PhD/Postdoc grant
    This research received funding from the Flemish Government (AI Research Program).
    EU H2020 ICT-48 Network TAILOR (Foundations of Trustworthy AI - Integrating Reasoning, Learning and Optimization)
    Leuven.AI Institute
  </p>
</div>

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
  <p style="text-align:center">
  &#169; This webpage was in part inspired from this
  <a href="https://github.com/monniert/project-webpage">template</a>.
  </p>
</div>

</body>
</html> 
